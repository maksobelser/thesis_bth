{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2a6e4425-e695-49fd-8924-a92a240f0a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import chex\n",
    "from dataclasses import field\n",
    "from typing import Dict, Iterable\n",
    "from scvi._types import LossRecord, Tensor\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def prepare_metadata(meta_data: pd.DataFrame,\n",
    "                     cov_cat_keys: Optional[list] = None,\n",
    "                     cov_cat_embed_keys: Optional[list] = None,\n",
    "                     cov_cont_keys: Optional[list] = None,\n",
    "                     orders=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param meta_data: Dataframe containing species and covariate info, e.g. from non-registered adata.obs\n",
    "    :param cov_cat_keys: List of categorical covariates column names.\n",
    "    :param cov_cat_embed_keys: List of categorical covariates column names to be encoded via embedding\n",
    "    rather than one-hot encoding.\n",
    "    :param cov_cont_keys: List of continuous covariates column names.\n",
    "    :param orders: Defined orders for species or categorical covariates. Dict with keys being\n",
    "    'species' or categorical covariates names and values being lists of categories. May contain more/less\n",
    "    categories than data.\n",
    "    :return: covariate data, dict with order of categories per covariate, dict with keys categorical and continuous\n",
    "    specifying lists of covariates\n",
    "    \"\"\"\n",
    "    if cov_cat_keys is None:\n",
    "        cov_cat_keys = []\n",
    "    if cov_cat_embed_keys is None:\n",
    "        cov_cat_embed_keys = []\n",
    "    if cov_cont_keys is None:\n",
    "        cov_cont_keys = []\n",
    "\n",
    "    def order_categories(values: pd.Series, categories: Union[List, None] = None):\n",
    "        if categories is None:\n",
    "            categories = pd.Categorical(values).categories.values\n",
    "        else:\n",
    "            missing = set(values.unique()) - set(categories)\n",
    "            if len(missing) > 0:\n",
    "                raise ValueError(f'Some values of {values.name} are not in the specified categories order: {missing}')\n",
    "        return list(categories)\n",
    "\n",
    "    def dummies_categories(values: pd.Series, categories: Union[List, None] = None):\n",
    "        \"\"\"\n",
    "        Make dummies of categorical covariates. Use specified order of categories.\n",
    "        :param values: Categories for each observation.\n",
    "        :param categories: Order of categories to use.\n",
    "        :return: dummies, categories. Dummies - one-hot encoding of categories in same order as categories.\n",
    "        \"\"\"\n",
    "        categories = order_categories(values=values, categories=categories)\n",
    "\n",
    "        # Get dummies\n",
    "        # Ensure ordering\n",
    "        values = pd.Series(pd.Categorical(values=values, categories=categories, ordered=True),\n",
    "                           index=values.index, name=values.name)\n",
    "        # This is problematic if many covariates\n",
    "        dummies = pd.get_dummies(values, prefix=values.name)\n",
    "\n",
    "        return dummies, categories\n",
    "\n",
    "    # Covariate encoding\n",
    "    # Save order of covariates and categories\n",
    "    cov_dict = {'categorical': cov_cat_keys, 'categorical_embed': cov_cat_embed_keys, 'continuous': cov_cont_keys}\n",
    "    # One-hot encoding of categorical covariates\n",
    "    orders_dict = {}\n",
    "\n",
    "    if len(cov_cat_keys) > 0 or len(cov_cont_keys) > 0:\n",
    "        cov_cat_data = []\n",
    "        for cov_cat_key in cov_cat_keys:\n",
    "            cat_dummies, cat_order = dummies_categories(\n",
    "                values=meta_data[cov_cat_key], categories=orders.get(cov_cat_key, None))\n",
    "            cov_cat_data.append(cat_dummies)\n",
    "            orders_dict[cov_cat_key] = cat_order\n",
    "        # Prepare single cov array for all covariates\n",
    "        cov_data_parsed = pd.concat(cov_cat_data + [meta_data[cov_cont_keys]], axis=1)\n",
    "    else:\n",
    "        cov_data_parsed = None\n",
    "\n",
    "    if len(cov_cat_embed_keys) > 0:\n",
    "        cov_embed_data = []\n",
    "        for cov_cat_embed_key in cov_cat_embed_keys:\n",
    "            cat_order = order_categories(values=meta_data[cov_cat_embed_key],\n",
    "                                         categories=orders.get(cov_cat_embed_key, None))\n",
    "            cat_map = dict(zip(cat_order, range(len(cat_order))))\n",
    "            cov_embed_data.append(meta_data[cov_cat_embed_key].map(cat_map))\n",
    "            orders_dict[cov_cat_embed_key] = cat_order\n",
    "        cov_embed_data = pd.concat(cov_embed_data, axis=1)\n",
    "    else:\n",
    "        cov_embed_data = None\n",
    "\n",
    "    return cov_data_parsed, cov_embed_data, orders_dict, cov_dict\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "def check_adatas_var_index(*adatas):\n",
    "    \"\"\"Check if the variable indices of all provided AnnData objects are the same and in the same order.\"\"\"\n",
    "    for i, j in combinations(range(len(adatas)), 2):\n",
    "        if not all(adatas[i].var.index == adatas[j].var.index):\n",
    "            raise ValueError(f\"The variable indices of the AnnData objects at positions {i} and {j} do not match!\")\n",
    "    print(\"Everything ok!\")\n",
    "    \n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def compute_r2_score(preds, ground_truth):\n",
    "    # Convert to densdataset adata.X is sparse\n",
    "    ground_truth = ground_truth.X.toarray()\n",
    "    ground_truth = ground_truth.mean(axis = 0)\n",
    "    preds = preds.X.mean(axis = 0)\n",
    "    # Compute R2 score\n",
    "    r2 = r2_score(ground_truth, preds)\n",
    "    return r2\n",
    "\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "\n",
    "def compare_de(X: np.ndarray, Y: np.ndarray, C: np.ndarray, shared_top: int = 100, **kwargs) -> dict:\n",
    "    \"\"\"Compare DEG across real and simulated perturbations.\n",
    "\n",
    "    Computes DEG for real and simulated perturbations vs. control and calculates\n",
    "    metrics to evaluate similarity of the results.\n",
    "\n",
    "    Args:\n",
    "        X: Real perturbed data.\n",
    "        Y: Simulated perturbed data.\n",
    "        C: Control data\n",
    "        shared_top: The number of top DEG to compute the proportion of their intersection.\n",
    "        **kwargs: arguments for `scanpy.tl.rank_genes_groups`.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_vars = X.shape[1]\n",
    "    assert n_vars == Y.shape[1] == C.shape[1]\n",
    "\n",
    "    shared_top = min(shared_top, n_vars)\n",
    "    vars_ranks = np.arange(1, n_vars + 1)\n",
    "\n",
    "    adatas_xy = {}\n",
    "    adatas_xy[\"x\"] = ad.AnnData(X, obs={\"label\": \"comp\"})\n",
    "    adatas_xy[\"y\"] = ad.AnnData(Y, obs={\"label\": \"comp\"})\n",
    "    adata_c = ad.AnnData(C, obs={\"label\": \"ctrl\"})\n",
    "\n",
    "    results = pd.DataFrame(index=adata_c.var_names)\n",
    "    top_names = []\n",
    "    for group in (\"x\", \"y\"):\n",
    "        adata_joint = ad.concat((adatas_xy[group], adata_c), index_unique=\"-\")\n",
    "\n",
    "        sc.tl.rank_genes_groups(adata_joint, groupby=\"label\", reference=\"ctrl\", key_added=\"de\", **kwargs)\n",
    "\n",
    "        srt_idx = np.argsort(adata_joint.uns[\"de\"][\"names\"][\"comp\"])\n",
    "        results[f\"scores_{group}\"] = adata_joint.uns[\"de\"][\"scores\"][\"comp\"][srt_idx]\n",
    "        results[f\"pvals_adj_{group}\"] = adata_joint.uns[\"de\"][\"pvals_adj\"][\"comp\"][srt_idx]\n",
    "        # needed to avoid checking rankby_abs\n",
    "        results[f\"ranks_{group}\"] = vars_ranks[srt_idx]\n",
    "\n",
    "        top_names.append(adata_joint.uns[\"de\"][\"names\"][\"comp\"][:shared_top])\n",
    "\n",
    "    metrics = {}\n",
    "    metrics[\"shared_top_genes\"] = len(set(top_names[0]).intersection(top_names[1])) / shared_top\n",
    "    metrics[\"scores_corr\"] = results[\"scores_x\"].corr(results[\"scores_y\"], method=\"pearson\")\n",
    "    metrics[\"pvals_adj_corr\"] = results[\"pvals_adj_x\"].corr(results[\"pvals_adj_y\"], method=\"pearson\")\n",
    "    metrics[\"scores_ranks_corr\"] = results[\"ranks_x\"].corr(results[\"ranks_y\"], method=\"spearman\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def compare_logfold(X: np.ndarray, Y: np.ndarray, C: np.ndarray, **kwargs) -> dict:\n",
    "    \"\"\"Compare DEG across real and simulated perturbations.\n",
    "\n",
    "    Computes DEG for real and simulated perturbations vs. control and calculates\n",
    "    metrics to evaluate similarity of the results.\n",
    "\n",
    "    Args:\n",
    "        X: Real perturbed data.\n",
    "        Y: Simulated perturbed data.\n",
    "        C: Control data\n",
    "        shared_top: The number of top DEG to compute the proportion of their intersection.\n",
    "        **kwargs: arguments for `scanpy.tl.rank_genes_groups`.\n",
    "    \"\"\"\n",
    "    n_vars = X.shape[1]\n",
    "    assert n_vars == Y.shape[1] == C.shape[1]\n",
    "\n",
    "    prop_of_genes_set_to_0 = np.mean(Y < 0)\n",
    "    Y[Y < 0] = 0\n",
    "\n",
    "    adatas_xy = {}\n",
    "    adatas_xy[\"x\"] = ad.AnnData(X, obs={\"label\": \"comp\"})\n",
    "    adatas_xy[\"y\"] = ad.AnnData(Y, obs={\"label\": \"comp\"})\n",
    "    adata_c = ad.AnnData(C, obs={\"label\": \"ctrl\"})\n",
    "\n",
    "    results = pd.DataFrame(index=adata_c.var_names)\n",
    "    top_names = []\n",
    "    for group in (\"x\", \"y\"):\n",
    "        adata_joint = ad.concat((adatas_xy[group], adata_c), index_unique=\"-\")\n",
    "\n",
    "        sc.tl.rank_genes_groups(adata_joint, groupby=\"label\", reference=\"ctrl\", key_added=\"de\", **kwargs)\n",
    "        results[f\"logfold_{group}\"] = [elm[0] for elm in adata_joint.uns[\"de\"][\"logfoldchanges\"].tolist()]\n",
    "\n",
    "    metics = {}\n",
    "    metics[\"logfold_corr\"] = results[\"logfold_x\"].corr(results[\"logfold_y\"], method=\"pearson\")\n",
    "    metics[\"prop_of_genes_set_to_0\"] = prop_of_genes_set_to_0\n",
    "\n",
    "    return metics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "67cf7e63-6386-465e-a775-2e9ccc3a6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from inspect import signature\n",
    "from typing import Any, Callable, Dict, Iterable, Literal, Optional, Union\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from scvi import METRIC_KEYS, REGISTRY_KEYS\n",
    "from scvi.autotune._types import Tunable, TunableMixin\n",
    "from scvi.module import Classifier\n",
    "from scvi.module.base import (\n",
    "    BaseModuleClass,\n",
    "    LossOutput,\n",
    ")\n",
    "\n",
    "from scvi.train._metrics import ElboMetric\n",
    "\n",
    "TorchOptimizerCreator = Callable[[Iterable[torch.Tensor]], torch.optim.Optimizer]\n",
    "\n",
    "def _compute_kl_weight(\n",
    "    epoch: int,\n",
    "    step: int,\n",
    "    n_epochs_kl_warmup: Optional[int],\n",
    "    n_steps_kl_warmup: Optional[int],\n",
    "    max_kl_weight: float = 1.0,\n",
    "    min_kl_weight: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Computes the kl weight for the current step or epoch.\n",
    "\n",
    "    If both `n_epochs_kl_warmup` and `n_steps_kl_warmup` are None `max_kl_weight` is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch\n",
    "        Current epoch.\n",
    "    step\n",
    "        Current step.\n",
    "    n_epochs_kl_warmup\n",
    "        Number of training epochs to scale weight on KL divergences from\n",
    "        `min_kl_weight` to `max_kl_weight`\n",
    "    n_steps_kl_warmup\n",
    "        Number of training steps (minibatches) to scale weight on KL divergences from\n",
    "        `min_kl_weight` to `max_kl_weight`\n",
    "    max_kl_weight\n",
    "        Maximum scaling factor on KL divergence during training.\n",
    "    min_kl_weight\n",
    "        Minimum scaling factor on KL divergence during training.\n",
    "    \"\"\"\n",
    "    if min_kl_weight > max_kl_weight:\n",
    "        raise ValueError(\n",
    "            f\"min_kl_weight={min_kl_weight} is larger than max_kl_weight={max_kl_weight}.\"\n",
    "        )\n",
    "\n",
    "    slope = max_kl_weight - min_kl_weight\n",
    "    if n_epochs_kl_warmup:\n",
    "        if epoch < n_epochs_kl_warmup:\n",
    "            return slope * (epoch / n_epochs_kl_warmup) + min_kl_weight\n",
    "    elif n_steps_kl_warmup:\n",
    "        if step < n_steps_kl_warmup:\n",
    "            return slope * (step / n_steps_kl_warmup) + min_kl_weight\n",
    "    return max_kl_weight\n",
    "\n",
    "\n",
    "class TrainingPlan(TunableMixin, pl.LightningModule):\n",
    "    \"\"\"Lightning module task to train scvi-tools modules.\n",
    "\n",
    "    The training plan is a PyTorch Lightning Module that is initialized\n",
    "    with a scvi-tools module object. It configures the optimizers, defines\n",
    "    the training step and validation step, and computes metrics to be recorded\n",
    "    during training. The training step and validation step are functions that\n",
    "    take data, run it through the model and return the loss, which will then\n",
    "    be used to optimize the model parameters in the Trainer. Overall, custom\n",
    "    training plans can be used to develop complex inference schemes on top of\n",
    "    modules.\n",
    "\n",
    "    The following developer tutorial will familiarize you more with training plans\n",
    "    and how to use them: :doc:`/tutorials/notebooks/dev/model_user_guide`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module\n",
    "        A module instance from class ``BaseModuleClass``.\n",
    "    optimizer\n",
    "        One of \"Adam\" (:class:`~torch.optim.Adam`), \"AdamW\" (:class:`~torch.optim.AdamW`),\n",
    "        or \"Custom\", which requires a custom optimizer creator callable to be passed via\n",
    "        `optimizer_creator`.\n",
    "    optimizer_creator\n",
    "        A callable taking in parameters and returning a :class:`~torch.optim.Optimizer`.\n",
    "        This allows using any PyTorch optimizer with custom hyperparameters.\n",
    "    lr\n",
    "        Learning rate used for optimization, when `optimizer_creator` is None.\n",
    "    weight_decay\n",
    "        Weight decay used in optimization, when `optimizer_creator` is None.\n",
    "    eps\n",
    "        eps used for optimization, when `optimizer_creator` is None.\n",
    "    n_steps_kl_warmup\n",
    "        Number of training steps (minibatches) to scale weight on KL divergences from\n",
    "        `min_kl_weight` to `max_kl_weight`. Only activated when `n_epochs_kl_warmup` is\n",
    "        set to None.\n",
    "    n_epochs_kl_warmup\n",
    "        Number of epochs to scale weight on KL divergences from `min_kl_weight` to\n",
    "        `max_kl_weight`. Overrides `n_steps_kl_warmup` when both are not `None`.\n",
    "    reduce_lr_on_plateau\n",
    "        Whether to monitor validation loss and reduce learning rate when validation set\n",
    "        `lr_scheduler_metric` plateaus.\n",
    "    lr_factor\n",
    "        Factor to reduce learning rate.\n",
    "    lr_patience\n",
    "        Number of epochs with no improvement after which learning rate will be reduced.\n",
    "    lr_threshold\n",
    "        Threshold for measuring the new optimum.\n",
    "    lr_scheduler_metric\n",
    "        Which metric to track for learning rate reduction.\n",
    "    lr_min\n",
    "        Minimum learning rate allowed.\n",
    "    max_kl_weight\n",
    "        Maximum scaling factor on KL divergence during training.\n",
    "    min_kl_weight\n",
    "        Minimum scaling factor on KL divergence during training.\n",
    "    **loss_kwargs\n",
    "        Keyword args to pass to the loss method of the `module`.\n",
    "        `kl_weight` should not be passed here and is handled automatically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        module: BaseModuleClass,\n",
    "        *,\n",
    "        optimizer: Tunable[Literal[\"Adam\", \"AdamW\", \"Custom\"]] = \"Adam\",\n",
    "        optimizer_creator: Optional[TorchOptimizerCreator] = None,\n",
    "        lr: Tunable[float] = 1e-3,\n",
    "        weight_decay: Tunable[float] = 1e-6,\n",
    "        eps: Tunable[float] = 0.01,\n",
    "        n_steps_kl_warmup: Tunable[int] = None,\n",
    "        n_epochs_kl_warmup: Tunable[int] = 400,\n",
    "        reduce_lr_on_plateau: Tunable[bool] = False,\n",
    "        lr_factor: Tunable[float] = 0.6,\n",
    "        lr_patience: Tunable[int] = 30,\n",
    "        lr_threshold: Tunable[float] = 0.0,\n",
    "        lr_scheduler_metric: Literal[\n",
    "            \"elbo_validation\", \"reconstruction_loss_validation\", \"kl_local_validation\"\n",
    "        ] = \"elbo_validation\",\n",
    "        lr_min: Tunable[float] = 0,\n",
    "        max_kl_weight: Tunable[float] = 1.0,\n",
    "        min_kl_weight: Tunable[float] = 0.0,\n",
    "        **loss_kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.eps = eps\n",
    "        self.optimizer_name = optimizer\n",
    "        self.n_steps_kl_warmup = n_steps_kl_warmup\n",
    "        self.n_epochs_kl_warmup = n_epochs_kl_warmup\n",
    "        self.reduce_lr_on_plateau = reduce_lr_on_plateau\n",
    "        self.lr_factor = lr_factor\n",
    "        self.lr_patience = lr_patience\n",
    "        self.lr_scheduler_metric = lr_scheduler_metric\n",
    "        self.lr_threshold = lr_threshold\n",
    "        self.lr_min = lr_min\n",
    "        self.loss_kwargs = loss_kwargs\n",
    "        self.min_kl_weight = min_kl_weight\n",
    "        self.max_kl_weight = max_kl_weight\n",
    "        self.optimizer_creator = optimizer_creator\n",
    "\n",
    "        if self.optimizer_name == \"Custom\" and self.optimizer_creator is None:\n",
    "            raise ValueError(\n",
    "                \"If optimizer is 'Custom', `optimizer_creator` must be provided.\"\n",
    "            )\n",
    "\n",
    "        self._n_obs_training = None\n",
    "        self._n_obs_validation = None\n",
    "\n",
    "        # automatic handling of kl weight\n",
    "        self._loss_args = set(signature(self.module.loss).parameters.keys())\n",
    "        if \"kl_weight\" in self._loss_args:\n",
    "            self.loss_kwargs.update({\"kl_weight\": self.kl_weight})\n",
    "\n",
    "        self.initialize_train_metrics()\n",
    "        self.initialize_val_metrics()\n",
    "\n",
    "    @staticmethod\n",
    "    def _create_elbo_metric_components(mode: str, n_total: Optional[int] = None):\n",
    "        \"\"\"Initialize ELBO metric and the metric collection.\"\"\"\n",
    "        rec_loss = ElboMetric(\"reconstruction_loss\", mode, \"obs\")\n",
    "        kl_local = ElboMetric(\"kl_local\", mode, \"obs\")\n",
    "        kl_global = ElboMetric(\"kl_global\", mode, \"batch\")\n",
    "        # n_total can be 0 if there is no validation set, this won't ever be used\n",
    "        # in that case anyway\n",
    "        n = 1 if n_total is None or n_total < 1 else n_total\n",
    "        elbo = rec_loss + kl_local + (1 / n) * kl_global\n",
    "        elbo.name = f\"elbo_{mode}\"\n",
    "        collection = OrderedDict(\n",
    "            [(metric.name, metric) for metric in [elbo, rec_loss, kl_local, kl_global]]\n",
    "        )\n",
    "        return elbo, rec_loss, kl_local, kl_global, collection\n",
    "\n",
    "    def initialize_train_metrics(self):\n",
    "        \"\"\"Initialize train related metrics.\"\"\"\n",
    "        (\n",
    "            self.elbo_train,\n",
    "            self.rec_loss_train,\n",
    "            self.kl_local_train,\n",
    "            self.kl_global_train,\n",
    "            self.train_metrics,\n",
    "        ) = self._create_elbo_metric_components(\n",
    "            mode=\"train\", n_total=self.n_obs_training\n",
    "        )\n",
    "        self.elbo_train.reset()\n",
    "\n",
    "    def initialize_val_metrics(self):\n",
    "        \"\"\"Initialize val related metrics.\"\"\"\n",
    "        (\n",
    "            self.elbo_val,\n",
    "            self.rec_loss_val,\n",
    "            self.kl_local_val,\n",
    "            self.kl_global_val,\n",
    "            self.val_metrics,\n",
    "        ) = self._create_elbo_metric_components(\n",
    "            mode=\"validation\", n_total=self.n_obs_validation\n",
    "        )\n",
    "        self.elbo_val.reset()\n",
    "\n",
    "    @property\n",
    "    def n_obs_training(self):\n",
    "        \"\"\"Number of observations in the training set.\n",
    "\n",
    "        This will update the loss kwargs for loss rescaling.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This can get set after initialization\n",
    "        \"\"\"\n",
    "        return self._n_obs_training\n",
    "\n",
    "    @n_obs_training.setter\n",
    "    def n_obs_training(self, n_obs: int):\n",
    "        if \"n_obs\" in self._loss_args:\n",
    "            self.loss_kwargs.update({\"n_obs\": n_obs})\n",
    "        self._n_obs_training = n_obs\n",
    "        self.initialize_train_metrics()\n",
    "\n",
    "    @property\n",
    "    def n_obs_validation(self):\n",
    "        \"\"\"Number of observations in the validation set.\n",
    "\n",
    "        This will update the loss kwargs for loss rescaling.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This can get set after initialization\n",
    "        \"\"\"\n",
    "        return self._n_obs_validation\n",
    "\n",
    "    @n_obs_validation.setter\n",
    "    def n_obs_validation(self, n_obs: int):\n",
    "        self._n_obs_validation = n_obs\n",
    "        self.initialize_val_metrics()\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Passthrough to the module's forward method.\"\"\"\n",
    "        return self.module(*args, **kwargs)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def compute_and_log_metrics(\n",
    "        self,\n",
    "        loss_output: LossOutput,\n",
    "        metrics: Dict[str, ElboMetric],\n",
    "        mode: str,\n",
    "        metrics_eval: Dict = None,\n",
    "    ):\n",
    "        \"\"\"Computes and logs metrics.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_output\n",
    "            LossOutput object from scvi-tools module\n",
    "        metrics\n",
    "            Dictionary of metrics to update\n",
    "        mode\n",
    "            Postfix string to add to the metric name of\n",
    "            extra metrics\n",
    "        \"\"\"\n",
    "        rec_loss = loss_output.reconstruction_loss_sum\n",
    "        n_obs_minibatch = loss_output.n_obs_minibatch\n",
    "        kl_local = loss_output.kl_local_sum\n",
    "        kl_global = loss_output.kl_global_sum\n",
    "\n",
    "        # Use the torchmetric object for the ELBO\n",
    "        # We only need to update the ELBO metric\n",
    "        # As it's defined as a sum of the other metrics\n",
    "        metrics[f\"elbo_{mode}\"].update(\n",
    "            reconstruction_loss=rec_loss,\n",
    "            kl_local=kl_local,\n",
    "            kl_global=kl_global,\n",
    "            n_obs_minibatch=n_obs_minibatch,\n",
    "        )\n",
    "        # pytorch lightning handles everything with the torchmetric object\n",
    "        self.log_dict(\n",
    "            metrics,\n",
    "            batch_size=n_obs_minibatch,\n",
    "        )\n",
    "\n",
    "        # accumlate extra metrics passed to loss recorder\n",
    "        for key in loss_output.extra_metrics_keys:\n",
    "            met = loss_output.extra_metrics[key]\n",
    "            if isinstance(met, torch.Tensor):\n",
    "                if met.shape != torch.Size([]):\n",
    "                    raise ValueError(\"Extra tracked metrics should be 0-d tensors.\")\n",
    "                met = met.detach()\n",
    "            self.log(\n",
    "                f\"{key}_{mode}\",\n",
    "                met,\n",
    "                on_step=False,\n",
    "                on_epoch=True,\n",
    "                batch_size=n_obs_minibatch,\n",
    "            )\n",
    "        # accumulate extra eval metrics\n",
    "        if metrics_eval is not None:\n",
    "            for extra_metric, met in metrics_eval.items():\n",
    "                if isinstance(met, torch.Tensor):\n",
    "                    if met.shape != torch.Size([]):\n",
    "                        raise ValueError(\"Extra tracked metrics should be 0-d tensors.\")\n",
    "                    met = met.detach()\n",
    "                self.log(\n",
    "                    f\"{extra_metric}_{mode}_eval\",\n",
    "                    met,\n",
    "                    batch_size=n_obs_minibatch,\n",
    "                )\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step for the model.\"\"\"\n",
    "        if \"kl_weight\" in self.loss_kwargs:\n",
    "            kl_weight = self.kl_weight\n",
    "            self.loss_kwargs.update({\"kl_weight\": kl_weight})\n",
    "            self.log(\"kl_weight\", kl_weight, on_step=True, on_epoch=False)\n",
    "        _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
    "        self.log(\"train_loss\", scvi_loss.loss, on_epoch=True, prog_bar=True)\n",
    "        self.compute_and_log_metrics(scvi_loss, self.train_metrics, \"train\")\n",
    "        return scvi_loss.loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # loss kwargs here contains `n_obs` equal to n_training_obs\n",
    "        # so when relevant, the actual loss value is rescaled to number\n",
    "        # of training examples\n",
    "        _, _, scvi_loss = self.forward(batch, loss_kwargs=self.loss_kwargs)\n",
    "        # self.log(\"validation_loss\", scvi_loss.loss, on_epoch=True)  # Saved above via loss recorder\n",
    "        metrics_eval = self.module.compute_validation_metrics()\n",
    "        self.compute_and_log_metrics(scvi_loss, self.val_metrics, \"validation\", metrics_eval=metrics_eval)\n",
    "\n",
    "    def _optimizer_creator_fn(\n",
    "        self, optimizer_cls: Union[torch.optim.Adam, torch.optim.AdamW]\n",
    "    ):\n",
    "        \"\"\"Create optimizer for the model.\n",
    "\n",
    "        This type of function can be passed as the `optimizer_creator`\n",
    "        \"\"\"\n",
    "        return lambda params: optimizer_cls(\n",
    "            params, lr=self.lr, eps=self.eps, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "    def get_optimizer_creator(self):\n",
    "        \"\"\"Get optimizer creator for the model.\"\"\"\n",
    "        if self.optimizer_name == \"Adam\":\n",
    "            optim_creator = self._optimizer_creator_fn(torch.optim.Adam)\n",
    "        elif self.optimizer_name == \"AdamW\":\n",
    "            optim_creator = self._optimizer_creator_fn(torch.optim.AdamW)\n",
    "        elif self.optimizer_name == \"Custom\":\n",
    "            optim_creator = self.optimizer_creator\n",
    "        else:\n",
    "            raise ValueError(\"Optimizer not understood.\")\n",
    "\n",
    "        return optim_creator\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers for the model.\"\"\"\n",
    "        params = filter(lambda p: p.requires_grad, self.module.parameters())\n",
    "        optimizer = self.get_optimizer_creator()(params)\n",
    "        config = {\"optimizer\": optimizer}\n",
    "        if self.reduce_lr_on_plateau:\n",
    "            scheduler = ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                patience=self.lr_patience,\n",
    "                factor=self.lr_factor,\n",
    "                threshold=self.lr_threshold,\n",
    "                min_lr=self.lr_min,\n",
    "                threshold_mode=\"abs\",\n",
    "                verbose=True,\n",
    "            )\n",
    "            config.update(\n",
    "                {\n",
    "                    \"lr_scheduler\": {\n",
    "                        \"scheduler\": scheduler,\n",
    "                        \"monitor\": self.lr_scheduler_metric,\n",
    "                    },\n",
    "                },\n",
    "            )\n",
    "        return config\n",
    "\n",
    "    @property\n",
    "    def kl_weight(self):\n",
    "        \"\"\"Scaling factor on KL divergence during training.\"\"\"\n",
    "        return _compute_kl_weight(\n",
    "            self.current_epoch,\n",
    "            self.global_step,\n",
    "            self.n_epochs_kl_warmup,\n",
    "            self.n_steps_kl_warmup,\n",
    "            self.max_kl_weight,\n",
    "            self.min_kl_weight,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "39ed3e85-0af9-42e1-8f3f-6ddfc96db307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union\n",
    "\n",
    "from scvi.dataloaders import DataSplitter\n",
    "from scvi.model._utils import get_max_epochs_heuristic\n",
    "from scvi.train import TrainRunner#, TrainingPlan\n",
    "from scvi.utils._docstrings import devices_dsp\n",
    "#from transVAE.train._training_plan import TrainingPlan\n",
    "\n",
    "class UnsupervisedTrainingMixin:\n",
    "    \"\"\"General purpose unsupervised train method.\"\"\"\n",
    "\n",
    "    _data_splitter_cls = DataSplitter\n",
    "    _training_plan_cls = TrainingPlan\n",
    "    _train_runner_cls = TrainRunner\n",
    "\n",
    "    @devices_dsp.dedent\n",
    "    def train(\n",
    "        self,\n",
    "        max_epochs: Optional[int] = None,\n",
    "        use_gpu: Optional[Union[str, int, bool]] = None,\n",
    "        accelerator: str = \"auto\",\n",
    "        devices: Union[int, List[int], str] = \"auto\",\n",
    "        train_size: float = 0.9,\n",
    "        validation_size: Optional[float] = None,\n",
    "        shuffle_set_split: bool = True,\n",
    "        batch_size: int = 128,\n",
    "        early_stopping: bool = False,\n",
    "        plan_kwargs: Optional[dict] = None,\n",
    "        **trainer_kwargs,\n",
    "    ):\n",
    "        \"\"\"Train the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        max_epochs\n",
    "            Number of passes through the dataset. If `None`, defaults to\n",
    "            `np.min([round((20000 / n_cells) * 400), 400])`\n",
    "        %(param_use_gpu)s\n",
    "        %(param_accelerator)s\n",
    "        %(param_devices)s\n",
    "        train_size\n",
    "            Size of training set in the range [0.0, 1.0].\n",
    "        validation_size\n",
    "            Size of the test set. If `None`, defaults to 1 - `train_size`. If\n",
    "            `train_size + validation_size < 1`, the remaining cells belong to a test set.\n",
    "        shuffle_set_split\n",
    "            Whether to shuffle indices before splitting. If `False`, the val, train, and test set are split in the\n",
    "            sequential order of the data according to `validation_size` and `train_size` percentages.\n",
    "        batch_size\n",
    "            Minibatch size to use during training.\n",
    "        early_stopping\n",
    "            Perform early stopping. Additional arguments can be passed in `**kwargs`.\n",
    "            See :class:`~scvi.train.Trainer` for further options.\n",
    "        plan_kwargs\n",
    "            Keyword args for :class:`~scvi.train.TrainingPlan`. Keyword arguments passed to\n",
    "            `train()` will overwrite values present in `plan_kwargs`, when appropriate.\n",
    "        **trainer_kwargs\n",
    "            Other keyword args for :class:`~scvi.train.Trainer`.\n",
    "        \"\"\"\n",
    "        if max_epochs is None:\n",
    "            max_epochs = get_max_epochs_heuristic(self.adata.n_obs)\n",
    "\n",
    "        plan_kwargs = plan_kwargs if isinstance(plan_kwargs, dict) else {}\n",
    "\n",
    "        data_splitter = self._data_splitter_cls(\n",
    "            self.adata_manager,\n",
    "            train_size=train_size,\n",
    "            validation_size=validation_size,\n",
    "            batch_size=batch_size,\n",
    "            shuffle_set_split=shuffle_set_split,\n",
    "        )\n",
    "        training_plan = self._training_plan_cls(self.module, **plan_kwargs)\n",
    "\n",
    "        es = \"early_stopping\"\n",
    "        trainer_kwargs[es] = (\n",
    "            early_stopping if es not in trainer_kwargs.keys() else trainer_kwargs[es]\n",
    "        )\n",
    "        runner = self._train_runner_cls(\n",
    "            self,\n",
    "            training_plan=training_plan,\n",
    "            data_splitter=data_splitter,\n",
    "            max_epochs=max_epochs,\n",
    "            use_gpu=use_gpu,\n",
    "            accelerator=accelerator,\n",
    "            devices=devices,\n",
    "            **trainer_kwargs,\n",
    "        )\n",
    "        return runner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "b812cfc4-ed8c-4638-acac-562051e76f23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional, Dict\n",
    "import numpy as np\n",
    "from anndata import AnnData\n",
    "\n",
    "import torch\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import kl_divergence as kl\n",
    "\n",
    "from scvi import REGISTRY_KEYS\n",
    "from scvi.distributions import NegativeBinomial\n",
    "from scvi.module.base import BaseModuleClass, auto_move_data\n",
    "from scvi.module.base._base_module import LossOutput\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from transVAE.nn._base_components import Encoder, Decoder, Embedding\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Conditional VAE model\n",
    "class VAEC(BaseModuleClass):\n",
    "    \"\"\"Conditional Variational auto-encoder model.\n",
    "\n",
    "    This is an implementation of the CondSCVI model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input\n",
    "        Number of input genes\n",
    "    n_labels\n",
    "        Number of labels\n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer\n",
    "    n_latent\n",
    "        Dimensionality of the latent space\n",
    "    n_layers\n",
    "        Number of hidden layers used for encoder and decoder NNs\n",
    "    log_variational\n",
    "        Log(data+1) prior to encoding for numerical stability. Not normalization.\n",
    "    dropout_rate\n",
    "        Dropout rate for the encoder and decoder neural network.\n",
    "    extra_encoder_kwargs\n",
    "        Keyword arguments passed into :class:`~scvi.nn.Encoder`.\n",
    "    extra_decoder_kwargs\n",
    "        Keyword arguments passed into :class:`~scvi.nn.FCLayers`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_input: int,\n",
    "        n_hidden: int = 128,\n",
    "        n_latent: int = 5,\n",
    "        n_layers: int = 2,\n",
    "        log_variational: bool = True,\n",
    "        kl_weight: float = 0.005,\n",
    "        dropout_rate: float = 0.05,\n",
    "        px_decoder: bool = False, \n",
    "        cov_embed_dims: int = 10,\n",
    "        extra_encoder_kwargs: Optional[dict] = None,\n",
    "        extra_decoder_kwargs: Optional[dict] = None,\n",
    "        use_exponentiation: bool = True,\n",
    "        initialize_weights: bool = False,\n",
    "        validation_adatas_dict: Optional[dict] = None,\n",
    "        original_adata_covariates = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dispersion = \"gene\"\n",
    "        self.n_latent = n_latent\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.log_variational = log_variational\n",
    "        self.gene_likelihood = \"nb\"\n",
    "        self.latent_distribution = \"normal\"\n",
    "        self.px_decoder = px_decoder\n",
    "        # Automatically deactivate if useless\n",
    "        self.n_batch = 0\n",
    "        self.kl_weight = kl_weight\n",
    "        self.use_exponentiation = use_exponentiation\n",
    "        self.val_adatas = validation_adatas_dict\n",
    "        self.original_adata_covariates = original_adata_covariates\n",
    "                \n",
    "        # Initialize embeddings for covariates with high cardinality\n",
    "        if \"covariates_embed\" in adata.obsm.keys():\n",
    "            self.embed_cov_sizes = adata.obsm[\"covariates_embed\"].nunique().tolist()\n",
    "            self.cov_embeddings = torch.nn.ModuleList([\n",
    "                Embedding(size=size, cov_embed_dims=cov_embed_dims) for size in self.embed_cov_sizes])\n",
    "            self.embed_cov = True\n",
    "            self.n_cov_embed = len(adata.obsm[\"covariates_embed\"].columns)*cov_embed_dims\n",
    "        else:\n",
    "            self.embed_cov = False\n",
    "            self.n_cov_embed = 0\n",
    "        \n",
    "        if \"covariates\" in adata.obsm.keys():\n",
    "            self.n_cov = len(adata.obsm[\"covariates\"].columns)\n",
    "        else:\n",
    "            self.n_cov = 0\n",
    "            \n",
    "        self.n_cov_total = self.n_cov_embed + self.n_cov\n",
    "        if self.n_cov_total == 0:\n",
    "            self.n_cov_total = None \n",
    "\n",
    "        self.z_encoder = Encoder(\n",
    "            n_input=n_input,\n",
    "            n_output=n_latent,  \n",
    "            n_cat=self.n_cov_total,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate = dropout_rate,\n",
    "            var_eps=1e-4\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            n_input=n_latent,\n",
    "            n_output=n_input,\n",
    "            n_cat=self.n_cov_total,\n",
    "            n_layers=n_layers,\n",
    "            n_hidden=n_hidden,\n",
    "            dropout_rate = dropout_rate,\n",
    "            var_eps=1e-4\n",
    "        )\n",
    "        \n",
    "        if initialize_weights:\n",
    "            self.z_encoder.initialize_weights()\n",
    "            self.decoder.initialize_weights()\n",
    "        \n",
    "    def _get_cov(self, tensors):\n",
    "        \n",
    "        cov_list = []\n",
    "\n",
    "        # Check if 'covariates' key exists in tensors and its size\n",
    "        if 'covariates' in tensors and tensors['covariates'].shape[1] > 0:\n",
    "            cov_list.append(tensors['covariates'])\n",
    "        \n",
    "        # Check if 'covariates_embed' key exists in tensors\n",
    "        if self.embed_cov:\n",
    "            # Dynamically create embeddings if not already initialized\n",
    "            if not hasattr(self, 'cov_embeddings'):\n",
    "                self.embed_cov_sizes = [tensors['covariates_embed'][:, i].max().item() + 1 for i in range(tensors['covariates_embed'].shape[1])]\n",
    "                self.cov_embeddings = torch.nn.ModuleList([\n",
    "                    Embedding(size=size, cov_embed_dims=cov_embed_dims) for size in self.embed_cov_sizes])\n",
    "\n",
    "            # Append the embeddings\n",
    "            cov_list.extend([embedding(tensors['covariates_embed'][:, i].int()) \n",
    "                             for i, embedding in enumerate(self.cov_embeddings)])\n",
    "            \n",
    "        # Concatenate along dimension 1 or return None if empty\n",
    "        return torch.cat(cov_list, dim=1) if cov_list else None\n",
    "        \n",
    "    def _get_inference_input(self, tensors, **kwargs):\n",
    "        \"\"\"Parse the dictionary to get appropriate args\"\"\"\n",
    "        \n",
    "        expr = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        cov = self._get_cov(tensors=tensors)\n",
    "        input_dict = dict(expr=expr, cov=cov)\n",
    "        \n",
    "        return input_dict\n",
    "\n",
    "    def _get_generative_input(self, tensors, inference_outputs):\n",
    "        \"\"\"\n",
    "        Parse the dictionary to get appropriate args\n",
    "        :param cov_replace: Replace cov from tensors with this covariate vector (for predict)\n",
    "        \"\"\"\n",
    "\n",
    "        z = inference_outputs[\"z\"]\n",
    "        cov = self._get_cov(tensors=tensors)\n",
    "        input_dict = dict(z=z, cov=cov)\n",
    "        \n",
    "        return input_dict\n",
    "\n",
    "    @auto_move_data\n",
    "    def inference(self, expr, cov):\n",
    "        \"\"\"High level inference method.\n",
    "\n",
    "        Runs the inference (encoder) model.\n",
    "        \"\"\"\n",
    "        q_m, q_v, latent = self.z_encoder(expr, cov)        \n",
    "        outputs = {\"z\": latent, \"q_m\": q_m, \"q_v\": q_v}\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    @auto_move_data\n",
    "    def generative(self, z, cov):\n",
    "        \"\"\"Runs the generative model.\"\"\"\n",
    "        \n",
    "        p = self.decoder(z, cov)\n",
    "        return {\"px\": p}\n",
    "\n",
    "    def loss(\n",
    "        self,\n",
    "        tensors,\n",
    "        inference_outputs,\n",
    "        generative_outputs,\n",
    "        kl_weight: float = 0.0005,\n",
    "    ):\n",
    "        x = tensors[REGISTRY_KEYS.X_KEY]\n",
    "        qz_m = inference_outputs[\"q_m\"]\n",
    "        qz_v = inference_outputs[\"q_v\"]\n",
    "        p = generative_outputs[\"px\"]\n",
    "        \n",
    "        kld = kl(\n",
    "            Normal(qz_m, torch.sqrt(qz_v)),\n",
    "            Normal(0, 1),\n",
    "        ).sum(dim=1)           \n",
    "        \n",
    "        rl = self.get_reconstruction_loss(p, x)\n",
    "        loss = (0.5 * rl + 0.5 * (kld * self.kl_weight)).mean()\n",
    "\n",
    "        return LossOutput(loss=loss, reconstruction_loss=rl, kl_local=kld)\n",
    "        \n",
    "    def get_reconstruction_loss(self, x, px) -> torch.Tensor:\n",
    "        x = x[0] if isinstance(x, tuple) else x\n",
    "        px = torch.tensor(px) if not isinstance(px, torch.Tensor) else px\n",
    "        if self.use_exponentiation:\n",
    "            x = torch.exp(x) - 1\n",
    "            px = torch.exp(px) - 1\n",
    "            cap_value = 5000000\n",
    "            px = torch.clamp(px, min=None, max=cap_value)\n",
    "            px[torch.isinf(px)] = cap_value\n",
    "        loss = ((x - px) ** 2).sum(dim=1)\n",
    "        return loss\n",
    "\n",
    "    def compute_validation_metrics(self):\n",
    "        # device\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        validation_loss = {}\n",
    "        csdl = self.val_adatas[\"csdl_adata_to_predict\"]\n",
    "        \n",
    "        latent = []\n",
    "        qm = []\n",
    "        qv = []\n",
    "        expr = []\n",
    "        for tensors in csdl:\n",
    "            tensors = {k: v.to(device) for k, v in tensors.items()}  # Move tensors to the device\n",
    "            inference_inputs = self._get_inference_input(tensors)\n",
    "            outputs = self.inference(**inference_inputs)\n",
    "            z = outputs[\"z\"]\n",
    "            latent += [z]\n",
    "\n",
    "        latent = torch.cat(latent)\n",
    "        for name, data in self.val_adatas.items():\n",
    "            if name == \"csdl_adata_to_predict\":\n",
    "                continue\n",
    "            cov = self._get_cov(tensors=data[\"generative_tensors\"])\n",
    "            gt = data[\"ground_truth\"]\n",
    "            px = self.generative(z = latent, cov = cov)[\"px\"].cpu().detach().numpy().mean(axis = 0)\n",
    "            r2 = r2_score(gt, px)\n",
    "            \n",
    "            validation_loss[f\"{name}_r2\"] = r2\n",
    "        \n",
    "        if len(validation_loss) > 1:\n",
    "            validation_loss[\"mean_r2\"] = sum(validation_loss.values()) / len(validation_loss)\n",
    "        \n",
    "        return validation_loss\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "b035c237-0c89-4d35-aa59-489cde253677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scvi.data.fields import CategoricalObsField, LayerField, ObsmField\n",
    "from scvi.data import AnnDataManager\n",
    "from anndata import AnnData\n",
    "from scvi.utils import setup_anndata_dsp\n",
    "from scvi import REGISTRY_KEYS\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict, Sequence, Tuple\n",
    "import torch\n",
    "from scvi.module.base import auto_move_data\n",
    "\n",
    "#from transVAE.module._base_module import VAEC\n",
    "#from transVAE.module._utils import prepare_metadata\n",
    "from scvi.model.base import VAEMixin, BaseModelClass#, UnsupervisedTrainingMixin\n",
    "#from transVAE.model._training import UnsupervisedTrainingMixin\n",
    "\n",
    "class transVAE(VAEMixin, UnsupervisedTrainingMixin, BaseModelClass):\n",
    "    \"\"\"\n",
    "    Implementation of VAE model\n",
    "    Parameters\n",
    "    ----------\n",
    "    adata\n",
    "        AnnData object that has been registered. \n",
    "    n_hidden\n",
    "        Number of nodes per hidden layer.\n",
    "    n_latent\n",
    "        Dimensionality of the latent space.\n",
    "    n_layers\n",
    "        Number of hidden layers used for encoder and decoder NNs.\n",
    "    dropout_rate\n",
    "        Dropout rate for neural networks.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        n_labels: list = 0,\n",
    "        n_hidden: int = 800,\n",
    "        n_latent: int = 100,\n",
    "        n_layers: int = 2,\n",
    "        dropout_rate: float = 0.2,\n",
    "        cov_embed_dims: int = 10,\n",
    "        kl_weight: float = 0.005,\n",
    "        initialize_weights: bool = False,\n",
    "        use_exponentiation: bool = False,\n",
    "        validation_adatas_dict: Optional[Dict] = None,\n",
    "        **model_kwargs,\n",
    "    ):\n",
    "        super().__init__(adata)\n",
    "        # assign n_input\n",
    "        n_input = self.summary_stats.n_vars\n",
    "        self.use_exponentiation = use_exponentiation\n",
    "        if validation_adatas_dict is not None:\n",
    "            validation_adatas_dict = self.prepare_validation_data(validation_adatas_dict)\n",
    "        \n",
    "        # cVAE\n",
    "        self.module = VAEC(\n",
    "            adata = adata,\n",
    "            n_input=n_input,\n",
    "            n_hidden=n_hidden,\n",
    "            n_latent=n_latent,\n",
    "            n_layers=n_layers,\n",
    "            dropout_rate=dropout_rate,\n",
    "            cov_embed_dims = cov_embed_dims,\n",
    "            kl_weight = kl_weight,\n",
    "            initialize_weights = initialize_weights,\n",
    "            use_exponentiation = use_exponentiation,\n",
    "            validation_adatas_dict = validation_adatas_dict,\n",
    "            original_adata_covariates = self.adata.obsm,\n",
    "            **model_kwargs,\n",
    "        )\n",
    "        self._model_summary_string = (\n",
    "            \"Model with the following params: n_hidden: {}, n_latent: {}, n_layers: {}, dropout_rate: {}, n_labels {}\"\n",
    "        ).format(\n",
    "            n_hidden,\n",
    "            n_latent,\n",
    "            n_layers,\n",
    "            dropout_rate,\n",
    "            n_labels\n",
    "        )\n",
    "\n",
    "        self.init_params_ = self._get_init_params(locals())\n",
    "\n",
    "    def prepare_validation_data(self, \n",
    "                                validation_adatas_dict: Dict,\n",
    "                                batch_size = 4048) -> Dict:\n",
    "        \"\"\"\n",
    "        Prepares validation data for models that require embedding categorical covariates, \n",
    "        and validation of input AnnData objects. This method primarily processes and \n",
    "        augments `AnnData` objects with necessary metadata, embeddings, and data loaders \n",
    "        for downstream model validation.\n",
    "\n",
    "        The method operates in-place, modifying `validation_adatas_dict` to include \n",
    "        data loaders and processed AnnData objects. It relies on a 'control' key being \n",
    "        present within `validation_adatas_dict`, and 'translation dict' in the `obsm` \n",
    "        attribute of AnnData objects for embedding.\n",
    "\n",
    "        Parameters:\n",
    "        - validation_adatas_dict (dict): A dictionary where keys are descriptive names and \n",
    "          values are AnnData objects intended for validation. Must include keys 'adata_to_predict', \n",
    "          'translate_dicts', and 'ground_truths' for operations within the method.\n",
    "\n",
    "        Returns:\n",
    "        - dict: Updated validation_adatas_dict with data loaders and other relevant \n",
    "          data structures needed for validation.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If 'control' key is not present in `validation_adatas_dict`.\n",
    "        - KeyError: If a key from `translate_dicts` is not present in the `obs` attribute \n",
    "          of an AnnData object.\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        adata_to_predict = validation_adatas_dict[\"adata_to_predict\"]\n",
    "        translate_dicts = validation_adatas_dict[\"translate_dicts\"]\n",
    "        ground_truths = validation_adatas_dict[\"ground_truths\"]\n",
    "\n",
    "        # Extract covariate information from self.adata\n",
    "        categorical_covariate_keys = self.adata.uns[\"covariates_dict\"][\"categorical\"]\n",
    "        categorical_covariate_embed_keys = self.adata.uns[\"covariates_dict\"][\"categorical_embed\"]\n",
    "        orders = self.adata.uns[\"covariate_orders\"]\n",
    "\n",
    "        # Prepare metadata for validation AnnData\n",
    "        covariates, covariates_embed, orders_dict, cov_dict = prepare_metadata(\n",
    "            meta_data=adata_to_predict.obs,\n",
    "            cov_cat_keys=categorical_covariate_keys,\n",
    "            cov_cat_embed_keys=categorical_covariate_embed_keys,\n",
    "            orders=orders\n",
    "        )\n",
    "\n",
    "        # Update the uns and obsm of validation AnnData with new metadata and embeddings\n",
    "        adata_to_predict.uns['covariate_orders'] = orders_dict\n",
    "        adata_to_predict.uns['covariates_dict'] = cov_dict\n",
    "        if categorical_covariate_keys is not None:\n",
    "            adata_to_predict.obsm['covariates'] = covariates\n",
    "        if categorical_covariate_embed_keys is not None:\n",
    "            adata_to_predict.obsm['covariates_embed'] = covariates_embed\n",
    "\n",
    "        # Validate the AnnData object and create a DataLoader\n",
    "        adata_to_predict = self._validate_anndata(adata_to_predict)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata_to_predict, indices=None, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        validation_dict = dict()\n",
    "        validation_dict[\"csdl_adata_to_predict\"] = scdl\n",
    "        \n",
    "        for ground_truth_name, translate_dict in translate_dicts.items():\n",
    "            # latent variable switching\n",
    "            for column in translate_dict.keys():\n",
    "                if column not in list(adata_to_predict.obs.columns):\n",
    "                    raise KeyError(\"Dict key from translate_dict not found in adata.obs.\")\n",
    "                adata_to_predict.obs[column] = translate_dict[column]\n",
    "\n",
    "            covariates, covariates_embed, orders_dict, cov_dict = prepare_metadata(\n",
    "                meta_data=adata_to_predict.obs,\n",
    "                cov_cat_keys=categorical_covariate_keys,\n",
    "                cov_cat_embed_keys=categorical_covariate_embed_keys,\n",
    "                orders=self.adata.uns[\"covariate_orders\"]\n",
    "            )\n",
    "\n",
    "            tensors = {\n",
    "                \"covariates\": torch.Tensor(covariates.values).to(device), \n",
    "                \"covariates_embed\": torch.Tensor(covariates_embed.values).to(device)\n",
    "            }\n",
    "            \n",
    "            gt = ground_truths[ground_truth_name].X.toarray().mean(axis = 0)\n",
    "                \n",
    "            # Update the dictionary with the DataLoader\n",
    "            validation_dict[ground_truth_name] = {\"ground_truth\":gt, \"generative_tensors\": tensors}\n",
    "\n",
    "        return validation_dict\n",
    "    \n",
    "    @auto_move_data\n",
    "    @torch.inference_mode()\n",
    "    def get_latent_representation(\n",
    "        self,\n",
    "        adata: Optional[AnnData] = None,\n",
    "        indices: Optional[Sequence[int]] = None,\n",
    "        mc_samples: int = 5000,\n",
    "        batch_size: Optional[int] = None,\n",
    "        \n",
    "    ) -> np.ndarray:\n",
    "        \n",
    "        \"\"\"Return the latent representation for each cell.\n",
    "\n",
    "        This is typically denoted as :math:`z_n`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        adata\n",
    "            AnnData object with equivalent structure to initial AnnData. If `None`, defaults to the\n",
    "            AnnData object used to initialize the model.\n",
    "        indices\n",
    "            Indices of cells in adata to use. If `None`, all cells are used.\n",
    "        batch_size\n",
    "            Minibatch size for data loading into model. Defaults to `scvi.settings.batch_size`.\n",
    "        Returns\n",
    "        -------\n",
    "        Low-dimensional representation for each cell.\n",
    "        \"\"\"\n",
    "        \n",
    "        self._check_if_trained(warn=False)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        categorical_covariate_keys = self.adata.uns[\"covariates_dict\"][\"categorical\"]\n",
    "        categorical_covariate_embed_keys = self.adata.uns[\"covariates_dict\"][\"categorical_embed\"]\n",
    "        \n",
    "        covariates, covariates_embed, orders_dict, cov_dict = prepare_metadata(\n",
    "            meta_data=adata.obs,\n",
    "            cov_cat_keys=categorical_covariate_keys,\n",
    "            cov_cat_embed_keys=categorical_covariate_embed_keys,\n",
    "            orders=self.adata.uns[\"covariate_orders\"]\n",
    "        )\n",
    "        \n",
    "        adata.uns['covariate_orders'] = orders_dict\n",
    "        adata.uns['covariates_dict'] = cov_dict\n",
    "        if categorical_covariate_keys is not None:\n",
    "            if 'covariates' in adata.obsm:\n",
    "                del adata.obsm['covariates']\n",
    "            adata.obsm['covariates'] = covariates\n",
    "        if categorical_covariate_embed_keys is not None:\n",
    "            if 'covariates_embed' in adata.obsm:\n",
    "                del adata.obsm['covariates_embed']\n",
    "            adata.obsm['covariates_embed'] = covariates_embed\n",
    "        \n",
    "        adata = self._validate_anndata(adata)\n",
    "        scdl = self._make_data_loader(\n",
    "            adata=adata, indices=indices, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        latent = []\n",
    "        for tensors in scdl:\n",
    "            tensors = {k: v.to(device) for k, v in tensors.items()}  # Move tensors to the device\n",
    "            inference_inputs = self.module._get_inference_input(tensors)\n",
    "            outputs = self.module.inference(**inference_inputs)\n",
    "            z = outputs[\"z\"]\n",
    "            latent += [z.cpu()]\n",
    "            \n",
    "        return torch.cat(latent).numpy()\n",
    "    \n",
    "    @auto_move_data\n",
    "    def translate(\n",
    "        self,\n",
    "        adata: AnnData,\n",
    "        translate_dict: Dict,\n",
    "        copy: bool = False,\n",
    "        \n",
    "    ) -> AnnData:\n",
    "        \"\"\"\n",
    "        Translate the given adata based on the provided translation dictionary.\n",
    "\n",
    "        The function goes through an inference process to obtain latent representations \n",
    "        and then uses a generative process with latent varibale switching to predict cells. \n",
    "        The results are formatted and returned as an AnnData object.\n",
    "\n",
    "        Parameters:\n",
    "        - adata (AnnData): The input AnnData object.\n",
    "        - translate_dict (Dict): Dictionary specifying which column in the adata should be translated to which variable.\n",
    "        - copy (bool, optional): If True, a copy of the input adata will be used for processing. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - AnnData: An AnnData object containing the predicted cells.\n",
    "        \n",
    "        Examples:\n",
    "        \n",
    "        >> predicted = model.translate(adata_train, translate_dict= {\"dataset\": \"chem\"})\n",
    "        \n",
    "        \"\"\"\n",
    "        # find the device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Make sure var names are unique\n",
    "        if adata.shape[1] != len(set(adata.var_names)):\n",
    "            raise ValueError('Adata var_names are not unique')\n",
    "\n",
    "        if copy:\n",
    "            adata = adata.copy()\n",
    "            \n",
    "        ### Inference -----------\n",
    "                \n",
    "        latent = torch.Tensor(self.get_latent_representation(adata))\n",
    "            \n",
    "        ### Generative ----------\n",
    "        \n",
    "        categorical_covariate_keys = self.adata.uns[\"covariates_dict\"][\"categorical\"]\n",
    "        categorical_covariate_embed_keys = self.adata.uns[\"covariates_dict\"][\"categorical_embed\"]\n",
    "        \n",
    "        # latent variable switching\n",
    "        for column in translate_dict.keys():\n",
    "            if column not in list(adata.obs.columns):\n",
    "                raise KeyError(\"Dict key from translate_dict not found in adata.obs.\")\n",
    "            adata.obs[column] = translate_dict[column]\n",
    "        \n",
    "        covariates, covariates_embed, orders_dict, cov_dict = prepare_metadata(\n",
    "            meta_data=adata.obs,\n",
    "            cov_cat_keys=categorical_covariate_keys,\n",
    "            cov_cat_embed_keys=categorical_covariate_embed_keys,\n",
    "            orders=self.adata.uns[\"covariate_orders\"]\n",
    "        )\n",
    "            \n",
    "        tensors = {\n",
    "            \"covariates\": torch.Tensor(covariates.values).to(device), \n",
    "            \"covariates_embed\": torch.Tensor(covariates_embed.values).to(device)\n",
    "        }\n",
    "\n",
    "        cov = self.module._get_cov(tensors=tensors)\n",
    "        predicted_cells = self.module.generative(z = latent, cov = cov)[\"px\"].cpu().detach().numpy()\n",
    "        \n",
    "        # make output pretty\n",
    "        \n",
    "        adata.uns['covariate_orders'] = orders_dict\n",
    "        adata.uns['covariates_dict'] = cov_dict\n",
    "        if categorical_covariate_keys is not None:\n",
    "            if 'covariates' in adata.obsm:\n",
    "                del adata.obsm['covariates']\n",
    "            adata.obsm['covariates'] = covariates\n",
    "        if categorical_covariate_embed_keys is not None:\n",
    "            if 'covariates_embed' in adata.obsm:\n",
    "                del adata.obsm['covariates_embed']\n",
    "            adata.obsm['covariates_embed'] = covariates_embed\n",
    "        \n",
    "        predicted_adata = AnnData(\n",
    "            X=predicted_cells,\n",
    "            obs=adata.obs.copy(),\n",
    "            var=adata.var.copy(),\n",
    "            uns=adata.uns.copy(),\n",
    "            obsm=adata.obsm.copy(),\n",
    "        )\n",
    "        \n",
    "        return predicted_adata\n",
    "\n",
    "    @classmethod\n",
    "    @setup_anndata_dsp.dedent\n",
    "    def setup_anndata(\n",
    "        cls,\n",
    "        adata: AnnData,\n",
    "        categorical_covariate_keys: Optional[List[str]] = None,\n",
    "        categorical_covariate_embed_keys: Optional[List[str]] = None,\n",
    "        covariate_orders: Optional[Dict] = None,\n",
    "        copy: bool = True,\n",
    "        layer: Optional[str] = None,\n",
    "        validation_ind_key: Optional[str] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Sets up the AnnData object for subsequent analysis.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        cls : class\n",
    "            The class to which this classmethod belongs.\n",
    "        adata : AnnData\n",
    "            The annotated data matrix.\n",
    "        categorical_covariate_keys : Optional[List[str]], default=None\n",
    "            List of keys for categorical covariates.\n",
    "        categorical_covariate_embed_keys : Optional[List[str]], default=None\n",
    "            List of keys for categorical covariates to be embedded.\n",
    "        covariate_orders : Optional[Dict], default=None\n",
    "            Dictionary specifying the order of covariates.\n",
    "        copy : bool, default=True\n",
    "            Whether to return a copy of the original adata object.\n",
    "        layer : Optional[str], default=None\n",
    "            Specifies which layer of the adata object to consider.\n",
    "        **kwargs : Additional keyword arguments.\n",
    "\n",
    "        Returns:\n",
    "        -------\n",
    "        AnnData\n",
    "            The modified or copied AnnData object.\n",
    "\n",
    "        Raises:\n",
    "        ------\n",
    "        ValueError\n",
    "            If var_names in adata are not unique.\n",
    "        \"\"\"\n",
    "\n",
    "        # Make sure var names are unique\n",
    "        if adata.shape[1] != len(set(adata.var_names)):\n",
    "            raise ValueError('Adata var_names are not unique')\n",
    "\n",
    "        if copy:\n",
    "            adata = adata.copy()\n",
    "        \n",
    "        if covariate_orders is None:\n",
    "            covariate_orders = {}\n",
    "\n",
    "        covariates, covariates_embed, orders_dict, cov_dict = prepare_metadata(\n",
    "            meta_data=adata.obs,\n",
    "            cov_cat_keys=categorical_covariate_keys,\n",
    "            cov_cat_embed_keys=categorical_covariate_embed_keys,\n",
    "            orders=covariate_orders\n",
    "        )\n",
    "\n",
    "        adata.uns['covariate_orders'] = orders_dict\n",
    "        adata.uns['covariates_dict'] = cov_dict\n",
    "        if categorical_covariate_keys is not None:\n",
    "            if 'covariates' in adata.obsm:\n",
    "                del adata.obsm['covariates']\n",
    "            adata.obsm['covariates'] = covariates\n",
    "        if categorical_covariate_embed_keys is not None:\n",
    "            if 'covariates_embed' in adata.obsm:\n",
    "                del adata.obsm['covariates_embed']\n",
    "            adata.obsm['covariates_embed'] = covariates_embed\n",
    "\n",
    "        setup_method_args = cls._get_setup_method_args(**locals())\n",
    "        anndata_fields = [\n",
    "            LayerField(REGISTRY_KEYS.X_KEY, layer, is_count_data=False)\n",
    "        ]\n",
    "\n",
    "        if categorical_covariate_keys is not None:\n",
    "            anndata_fields.append(ObsmField('covariates', 'covariates'))\n",
    "        if categorical_covariate_embed_keys is not None:\n",
    "            anndata_fields.append(ObsmField('covariates_embed', 'covariates_embed'))\n",
    "\n",
    "        adata_manager = AnnDataManager(\n",
    "            fields=anndata_fields, setup_method_args=setup_method_args\n",
    "        )\n",
    "        adata_manager.register_fields(adata, **kwargs)\n",
    "        cls.register_manager(adata_manager)\n",
    "\n",
    "        return adata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ff37369a-5f2f-4f19-86df-ea562628e26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import scanpy as sc\n",
    "#adata = sc.read_h5ad(\"/d/hpc/projects/FRI/DL/mo6643/MSC/data/data_update_slack/data_splits/data_splits_train_merge/train_data/train_adata_baseline_3000hvg.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "71f5166c-a67c-491d-9fab-564391496934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Using column names from columns of adata.obsm\u001b[1m[\u001b[0m\u001b[32m'covariates'\u001b[0m\u001b[1m]\u001b[0m                                               \n",
      "\u001b[34mINFO    \u001b[0m Using column names from columns of adata.obsm\u001b[1m[\u001b[0m\u001b[32m'covariates_embed'\u001b[0m\u001b[1m]\u001b[0m                                         \n"
     ]
    }
   ],
   "source": [
    "adata_train = transVAE.setup_anndata(adata, categorical_covariate_embed_keys=[\"dataset\", \"cell_type\"], categorical_covariate_keys=[\"organism\"], copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "0be96088-4830-47a6-a6fc-5ed51b879241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adata_to_predict = sc.read_h5ad(f\"/d/hpc/projects/FRI/DL/mo6643/MSC/data/data_update_slack/data_splits/data_splits_train_merge/data_to_predict/wang_to_predict_3000hvg.h5ad\")\n",
    "#dbdb_ground_truth = sc.read_h5ad(f\"/d/hpc/projects/FRI/DL/mo6643/MSC/data/data_update_slack/data_splits/data_splits_train_merge/ground_truth/dbdb_ground_truth_3000hvg.h5ad\")\n",
    "#mSTZ_ground_truth = sc.read_h5ad(f\"/d/hpc/projects/FRI/DL/mo6643/MSC/data/data_update_slack/data_splits/data_splits_train_merge/ground_truth/mSTZ_ground_truth_3000hvg.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a560389-de15-4388-8a3c-ba78866278ec",
   "metadata": {},
   "source": [
    "Must include keys 'adata_to_predict','translate_dicts', and 'ground_truths' for operations within the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "3fe4a582-95c1-4e8e-b3d7-5d4c6f0b5763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_adatas_dict = {\"adata_to_predict\":adata_to_predict,\n",
    "                          \"ground_truths\": {\"dbdb\": dbdb_ground_truth,\n",
    "                                            \"mSTZ\": mSTZ_ground_truth},\n",
    "                          \"translate_dicts\": {\"dbdb\":{\"dataset\":\"db/db\", \"organism\": \"Mus musculus\"},\n",
    "                                              \"mSTZ\":{\"dataset\":\"mSTZ\", \"organism\": \"Mus musculus\"}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "96bae6d2-b739-4e65-91f1-e4f623ac1230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mINFO    \u001b[0m Input AnnData not setup with scvi-tools. attempting to transfer AnnData setup                             \n"
     ]
    }
   ],
   "source": [
    "# make the model\n",
    "model = transVAE(adata_train, \n",
    "                 n_hidden=1000, \n",
    "                 n_latent=256, \n",
    "                 n_layers=6, \n",
    "                 dropout_rate=0.3, \n",
    "                 cov_embed_dims = 10, \n",
    "                 kl_weight = 0.0005,\n",
    "                 use_exponentiation=False,\n",
    "                 validation_adatas_dict = validation_adatas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "b47c616c-4611-45bb-80fb-1728da7110f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100:   1%|          | 1/100 [00:07<12:37,  7.65s/it, v_num=1, train_loss_step=525, train_loss_epoch=568]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/d/hpc/home/mo6643/miniconda3/envs/msc1/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:53: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.train(batch_size=4096, max_epochs = 1000, train_size = 1, enable_progress_bar = False,\n",
    "            early_stopping = True, early_stopping_monitor = 'mean_r2_validation_eval', early_stopping_mode = \"max\", \n",
    "            early_stopping_min_delta = 0.01, early_stopping_patience = 70,\n",
    "            plan_kwargs = {\"lr\": 0.00001,\n",
    "                           \"weight_decay\":0.000001,\n",
    "                           \"reduce_lr_on_plateau\":True,\n",
    "                           \"lr_factor\":0.5,\n",
    "                           \"lr_patience\":50,\n",
    "                           \"lr_scheduler_metric\":\"reconstruction_loss_validation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "309fee2d-dce0-41bd-92e6-e9b143d2b4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kl_weight', 'train_loss_step', 'elbo_validation', 'reconstruction_loss_validation', 'kl_local_validation', 'kl_global_validation', 'dbdb_r2_validation_eval', 'mSTZ_r2_validation_eval', 'mean_r2_validation_eval', 'train_loss_epoch'])"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "3704aa14-4048-41c9-9ba4-61f2a319ac3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25259862434618824"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history[\"dbdb_r2_validation_eval\"].iloc[-1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce619e2-f8b2-4041-a00c-d6367b6f729b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c4937-b7c3-4903-aadd-941f6b707c70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
